{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal con Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la implementación computacional de redes neuronales, es común trabajar con grandes cantidades de datos en busca de la convergencia del método. Sin embargo, esto representa un gasto computacional considerable.\n",
    "\n",
    "La librería de theano permite optimizar este proceso. Esta librería ayuda a definir las expresiones matemáticas de una manera sencilla y al mismo tiempo optimiza el manejo de meoria cuando se trabajo con arreglos multi-dimensionales.\n",
    "\n",
    "Más sobre esta librería puede encontrarse en:\n",
    "\n",
    "http://deeplearning.net/software/theano/\n",
    "\n",
    "y en:\n",
    "\n",
    "https://arxiv.org/pdf/1605.02688.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente implementación en theano de una Feedforward Neural Network yoma como argumento los datos de ejmeplo (ex) y las etiquetas del supervisor (lab). Asimismo, diferentes parámetros pueden ser configurados: número de unidades ocultas, número de iteraciones (it), parámetro $\\eta$ (rate), y parámetro de regularización (reg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from theano import *\n",
    "\n",
    "\n",
    "def NN(ex,lab,hidden=1, its=1000, rate=0.01, reg=1):\n",
    "    #Se declaran las variables con que se trabajará\n",
    "    X,Y = T.dmatrices('X','Y')\n",
    "    y = T.dvector('y')\n",
    "    \n",
    "    #El total de clases que arrojará\n",
    "    out_dims= len(set(lab))\n",
    "    #El número de rasgos que representan cada vector\n",
    "    input_dims = ex.shape[1]\n",
    "    \n",
    "    #Con estos datos se generan los vectores de parámetros\n",
    "    np.random.seed(0)\n",
    "    W1 = shared(np.random.randn(input_dims, hidden)/np.sqrt(input_dims), name='W1' )\n",
    "    b1 = shared(np.zeros(hidden), name='b1')\n",
    "    W2 = shared(np.random.randn(hidden, out_dims)/np.sqrt(hidden), name='W2' )\n",
    "    b2 = shared(np.zeros(out_dims), name='b2')\n",
    "\n",
    "    #Defino la función Wx+b y la función de activación g=tanh\n",
    "    f1, updates = scan(lambda v: T.dot(v,W1) + b1, sequences=X )\n",
    "    g = T.tanh(f1)\n",
    "    f2, updates = scan(lambda v: T.dot(v,W2) + b2, sequences=g )\n",
    "    \n",
    "    #Uso la función Softmax para determinar las probabilidades de las clases\n",
    "    exp_scores = T.exp(f2)\n",
    "    prob = exp_scores / T.sum(exp_scores, axis=1, keepdims=True)\n",
    "    get_probs = function(inputs=[X], outputs=[prob,g])\n",
    "\n",
    "    #Defino la función de predicción que arrojará la clase con mayor probabilidad\n",
    "    predict = function(inputs=[X], outputs=[T.argmax(prob,axis=1)])\n",
    "\n",
    "    #Determinó las derivadas de mis funciones\n",
    "    dW2 = T.dot(Y.T,X)\n",
    "    db2 = T.sum(X, axis=0, keepdims=True)\n",
    "    d2 = T.dot(X, W2.T) * (1- Y**2)\n",
    "    dW1 = T.dot(ex.T, d2)\n",
    "    db1 = T.sum(d2, axis=0)\n",
    "\n",
    "    #Actualizo los parámetros de aprendizaje por medio de Stochastic Gradient Descent (SGD)\n",
    "    train = function(inputs=[X,Y], outputs=[dW1,db1,dW2,db2], updates=[(W2, W2-rate*(dW2 + reg*dW2)), (b2, b2-rate*db2[0]), (W1, W1-rate*(dW1 + reg*dW1)), (b1, b1-rate*db1) ])\n",
    "\n",
    "    #Itero el algoritmo\n",
    "    for i in range(0,its):\n",
    "        probs, activation = get_probs(ex)\n",
    "        probs[range(len(ex)), lab] -= 1\n",
    "        train(probs, activation)\n",
    "\n",
    "    return  predict, (W1.get_value(),b1.get_value(),W2.get_value(),b2.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos evaluar unos datos de ejmplo a partir de la red neuronal. En este caso, nos arrojará la función de predicción y el modelo aprendido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 2.44863591, -1.31374088,  2.65196629],\n",
      "       [ 2.45817634,  2.67103475, -1.58839646]]), array([-0.54382113,  0.19661476,  0.48807415]), array([[ 2.42088631, -1.95973863],\n",
      "       [-1.98607969,  2.16354542],\n",
      "       [-1.62913443,  2.55192323]]), array([ 0.02312234, -0.02312234]))\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,1],[1,0],[0,0],[1,1]])\n",
    "y = np.array([0,0,1,1])\n",
    "\n",
    "predict, model = NN(X,y,3)\n",
    "\n",
    "print model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la función de predicción obtenida, podemos evaluar nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1])]\n"
     ]
    }
   ],
   "source": [
    "z = [[0,0.2]]\n",
    "print predict(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
