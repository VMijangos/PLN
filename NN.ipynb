{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de una Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el modelo de la red neuronal se tiene comúnmente una función:\n",
    "        $$o(Wx+b)$$\n",
    "donde $W$ es un vector de pesos, $b$ es el bias y $o:\\mathbb{R}^n \\to \\{0,1\\}$ una función de salida que determina la clase del elemento al que pertenece el vector $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, se tomarán $m$ capas ocultas de tal forma que se tenga una función de pre-activación que definiremos como: $$a_i(x) = W_i\\cdot h_{i-1}(x)+b_i, i=1,...,m$$\n",
    "\n",
    "donde cada función de activación en las capas ocultas esté dada por $$h_i(x) = g(a_i(x))$$\n",
    "Ydonde $g \\equiv S$ donde la función $S: \\mathbb{R}^n \\to (0,1)$ es una función sigmoide definida como:\n",
    "$$S(x) = \\frac{1}{a+\\exp(-x)}$$ \n",
    "Aunque debe resaltarse que la función de activación puede ser de otro tipo, como la tangente hiperbólica. De está forma, la función de salida será: $$h_m(x) = o(a_m(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Definimos la función sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener la activación en la capa de salida, utilizaremos la función Softmax:\n",
    "$$f_c(x) = p(y=y_c|x) = \\frac{\\exp(W h_m(x)+b)}{\\sum_{x'}\\exp(W h_m(x')+b)}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtendremos la derivada de la función $-log(f(x)_c)$ como: $$\\frac{\\delta(-log(f(x)_c))}{\\delta a_i(x)_c} = f(x)_c - \\delta_{yc}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizará el algoritomo de Stochastic Gradient Descent (SGD) para actualizar los parámetros:\n",
    "$$ W_i \\leftarrow W_i - \\eta \\cdot \\nabla(-log(f(x)_c) + \\gamma \\cdot r(W_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(X, y, nn_hdim, its=10000, print_loss=False, reg_l=0.01, a=0.1):\n",
    "    np.random.seed(0)\n",
    "    #El número de rasgos que representan cada vector\n",
    "    nn_input_dim = X.shape[1]\n",
    "    #El total de clases que arrojará\n",
    "    output_dim = len(set(y))\n",
    "    #El número de ejmplos\n",
    "    num_examples = len(X)\n",
    "\n",
    "    #Con estos datos se generan los vectores de parámetros\n",
    "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    W2 = np.random.randn(nn_hdim,  output_dim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((1, output_dim ))\n",
    "    \n",
    "    model = {}\n",
    "    for i in xrange(0, its):\n",
    "        #Defino la función Wx+b y la función de activación g=tanh\n",
    "        z1 = np.dot(X,W1) + b1\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = np.dot(a1,W2) + b2\n",
    "\n",
    "        #Uso la función Softmax para determinar las probabilidades de las clases\n",
    "        exp_scores = np.exp(z2)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        #Cada clase correspondiente a los datos se le resta 1\n",
    "        probs[range(num_examples),y] -= 1\n",
    "\n",
    "        #Se obtienen las derivadas para los parámetros en las capas ocultas\n",
    "        dW2 = a1.T.dot(probs)\n",
    "        db2 = np.sum(probs,axis=0, keepdims=True)\n",
    "        \n",
    "        #Se obtienen las derivadas para los parámetros de entrada\n",
    "        d2 = probs.dot(W2.T) * (1-np.power(a1,2))\n",
    "        dW1 = np.dot(X.T,d2)\n",
    "        db1 = np.sum(d2, axis=0)\n",
    "        \n",
    "        #Se suma un valor de regularización a los parametros\n",
    "        dW2 += reg_l*W2\n",
    "        dW1 += reg_l*W1\n",
    "\n",
    "        #Se utiliza el método de estochastic Gradient Descent\n",
    "        W1 += -a*dW1\n",
    "        b1 += -a*db1\n",
    "        W2 += -a*dW2\n",
    "        b2 += -a*db2\n",
    "\n",
    "        model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print \"Perdida en la iteracion %i: %f\" %(i, loss(model))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez obtenido el modelo podemos obtener la función de predicción en la capa de salida de la red neuronal como:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_c p(y=y_c|x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    z1 = np.dot(X,W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1,W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos tomar algunos ejemplos para entrenar la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.65946476  -7.67624769]\n",
      " [ 10.22302928  10.24525071]]\n",
      "[[-6.52446281  1.12684182]]\n",
      "[[ 2.04374153 -2.04371309]\n",
      " [-2.04014573  2.04017124]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,1],[1,0],[0,0],[1,1]])\n",
    "y = np.array([0,0,1,1])\n",
    "\n",
    "model = build_model(X,y,2,print_loss=False)\n",
    "\n",
    "print model['W1']\n",
    "print model['b1']\n",
    "print model['W2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y finalmente, podemos evaluar un conjunto de puntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0,0])\n",
    "print predict(model, x)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
